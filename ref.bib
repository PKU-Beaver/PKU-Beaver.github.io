@software{leandro_von_werra_2023_7790115,
  author    = {Leandro von Werra and
               Jonathan Tow and
               reciprocated and
               Shahbuland Matiana and
               Alex Havrilla and
               cat-state and
               Louis Castricato and
               Alan and
               Duy V. Phung and
               Ayush Thakur and
               Alexey Bukhtiyarov and
               aaronrmm and
               Fabrizio Milo and
               Daniel and
               Daniel King and
               Dong Shin and
               Ethan Kim and
               Justin Wei and
               Manuel Romero and
               Nicky Pochinkov and
               Omar Sanseviero and
               Reshinth Adithyan and
               Sherman Siu and
               Thomas Simonini and
               Vladimir Blagojevic and
               Xu Song and
               Zack Witten and
               alexandremuzio and
               crumb},
  title     = {{CarperAI/trlx: v0.6.0: LLaMa (Alpaca), Benchmark Util, T5 ILQL, Tests}},
  month     = mar,
  year      = 2023,
  publisher = {Zenodo},
  version   = {v0.6.0},
  doi       = {10.5281/zenodo.7790115},
  url       = {https://github.com/CarperAI/trlx}
}

@software{deepspeed,
  author  = {microsoft},
  title   = {DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.},
  month   = may,
  year    = 2023,
  version = {v0.9.2},
  url     = {https://github.com/microsoft/DeepSpeed}
}

@article{ouyang2022training,
  title   = {Training language models to follow instructions with human feedback},
  author  = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {35},
  pages   = {27730--27744},
  year    = {2022}
}

@article{bai2022training,
  title   = {Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author  = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal = {arXiv preprint arXiv:2204.05862},
  year    = {2022}
}

@article{ganguli2022red,
  title   = {Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author  = {Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal = {arXiv preprint arXiv:2209.07858},
  year    = {2022}
}

@article{askell2021general,
  title   = {A general language assistant as a laboratory for alignment},
  author  = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal = {arXiv preprint arXiv:2112.00861},
  year    = {2021}
}
